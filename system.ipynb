{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abualia4/CloudFast-DL4PuDe/blob/main/system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DoefUAB4f9A"
      },
      "source": [
        "# Cloud-based Deep Learning System\n",
        "Inpus stream\n",
        "1. Web camera or any camera that connect to a web client.\n",
        "2. Virtual camera using a video recording of crowded event entrance; a video is in the sample directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sx0CW2ifcgK",
        "outputId": "a284d74e-fdcc-4342-bd35-779d1c0000c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Inputs"
      ],
      "metadata": {
        "id": "i2qQHwqpkIfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import output\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "from IPython.display import HTML\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from IPython.display import Image\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "from google.colab import output\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display\n",
        "import csv\n",
        "import time\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from collections import OrderedDict\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        " \n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras_preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras.preprocessing import image\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import seaborn as sns \n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "\n",
        "#################################################################################\n",
        "\n",
        "############################### User Inputs #####################################\n",
        "\n",
        "#################################################################################\n",
        "#The path of the system directory\n",
        "home_path='/content/drive/My Drive/Colab Notebooks/CD4EPD/system/'\n",
        "sys.path.insert(0,home_path)\n",
        "\n",
        "#The path of RAFT core directory\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/CD4EPD/system/RAFT/core')\n",
        "from raft import RAFT\n",
        "from utils import flow_viz\n",
        "\n",
        "#The dimensions of the input image for a classifier.\n",
        "img_rows, img_cols = 224,224\n",
        "\n",
        "#The path of the CNN classifier\n",
        "CNN_path=home_path+\"models/ourModel.h5\"\n",
        "\n",
        "#ROI coordinates\n",
        "roi=[107,55,669,270] #Entrance_2 video example\n",
        " \n",
        "# Rows x columns patches\n",
        "patch=[2,4] #Entrance_2 video example\n",
        "\n",
        "#In video_stream function, please determine the width and height of the monitoring window "
      ],
      "metadata": {
        "id": "gu2fxexrkHyO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8b8fb3q4uYa"
      },
      "source": [
        "# Libraries and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JfYYWx6aTeX"
      },
      "outputs": [],
      "source": [
        "#Loading the CNN classifier\n",
        "CNN_model= load_model(CNN_path)\n",
        "CNN_model.load_weights(CNN_path)\n",
        "\n",
        "#Image classification\n",
        "def classify(img):\n",
        "    img= img[...,::-1]\n",
        "    \n",
        "    im_pil = Image.fromarray(img)\n",
        "    im_resized = im_pil.resize((img_rows, img_cols),resample=0)\n",
        "    \n",
        "    img_array =img_to_array(  im_resized )\n",
        "    image_array_expanded = np.expand_dims(img_array, axis = 0)\n",
        "     \n",
        "    group=CNN_model.predict(image_array_expanded,verbose = 0 )\n",
        "    \n",
        "    if(group>=0.5):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#Determining the coordinates of the patches\n",
        "def patchBorder(rows, cols, left_top_x,left_top_y,right_bottom_x, right_bottom_y ):\n",
        "        #global x1\n",
        "        x1=[]\n",
        "        #global x2\n",
        "        x2=[]\n",
        "        #global y1\n",
        "        y1=[]\n",
        "        #global y2\n",
        "        y2=[]\n",
        "        \n",
        "        width=right_bottom_x-left_top_x\n",
        "        height=right_bottom_y-left_top_y\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                 \n",
        "                patch_width=int(width/cols)\n",
        "                patch_height=int(height/rows)\n",
        "            \n",
        "                left_x=left_top_x+patch_width*j\n",
        "                right_x=left_top_x+patch_width*(j+1)\n",
        "                left_y=left_top_y+patch_height*i\n",
        "                right_y=left_top_y+patch_height*(i+1)\n",
        "                \n",
        "                x1.append(left_x)\n",
        "                x2.append(right_x)\n",
        "                y1.append(left_y)\n",
        "                y2.append(right_y)\n",
        "         \n",
        "        if(x1[0]>x2[0]):\n",
        "            return(x2,y1,x1,y2)\n",
        "        else:\n",
        "            return(x1,y1,x2,y2)   \n",
        "\n",
        "\n",
        "#Updating ROI coordinates to fit tensor size in RAFT\n",
        "def new_roi(ratio, roi):\n",
        "    \n",
        "    w=roi[2]-roi[0]\n",
        "    h=roi[3]-roi[1]\n",
        "    new_w=int(w*ratio)\n",
        "    new_w=new_w-((new_w%24))\n",
        "    new_h=int(h*ratio)\n",
        "    new_h=new_h-((new_h%24))\n",
        "    \n",
        "    roi[0]=int(roi[0]*ratio)\n",
        "    roi[1]=int(roi[1]*ratio)\n",
        "    roi[2]=roi[0]+new_w\n",
        "    roi[3]=roi[1]+new_h\n",
        "   \n",
        "    return roi\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "#Live video stream from camera\n",
        "#JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "\n",
        "  js = Javascript('''\n",
        "  /////////////////////////////////////////////////\n",
        "  ///////////////////User Inputs///////////////////\n",
        "    var w=960 // window width\n",
        "    var h=540 // window height\n",
        "  ////////////////////////////////////////////////\n",
        "    var w_css=w+'px'\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, w, h);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "     \n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = w_css;\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span></span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = '';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = w; //video.videoWidth;\n",
        "      captureCanvas.height = h; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "      \n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    } \n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  \n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  \n",
        " \n",
        "  return data\n",
        "\n",
        "#Visualizing the optical flow vectors, and then generating the annotation mask.\n",
        "def vizualize_flow(img, flo, save, counter):\n",
        "   \n",
        "    img = img[0].permute(1, 2, 0).cpu().numpy()\n",
        "    flo = flo[0].permute(1, 2, 0).cpu().numpy()\n",
        "    flo = flow_viz.flow_to_image(flo) \n",
        "    flo = cv2.cvtColor(flo, cv2.COLOR_RGB2BGR)\n",
        "    image_blurred = cv2.blur(src=img, ksize=(10, 10))\n",
        "    labels=[]\n",
        "    for patch_no in range (0,len(x1)):  \n",
        "       patch=flo[y1[patch_no]:y2[patch_no],x1[patch_no]:x2[patch_no]]\n",
        "       if(classify(patch)==1):\n",
        "           border=[int(ox1[patch_no]),int(oy1[patch_no]),int(ox2[patch_no]),int(oy2[patch_no])]\n",
        "           labels.append(border )\n",
        "           cv2.rectangle(image_blurred, (int(x1[patch_no])+4,int(y1[patch_no])+7), (int(x2[patch_no]-4),int(y2[patch_no])-4), (0,0,255),3)\n",
        "    name=time.time()      \n",
        "    cv2.imwrite(home_path+\"/storage/\"+str(name)+\".png\", image_blurred)\n",
        "    return labels\n",
        "\n",
        "#Frame preprocessing\n",
        "def frame_preprocess(frame, device):\n",
        "    frame = torch.from_numpy(frame).permute(2, 0, 1).float()\n",
        "    frame = frame.unsqueeze(0)\n",
        "    frame = frame.to(device)\n",
        "    return frame\n",
        "\n",
        "#Frames extraction and processing, optical flow vectors estimation and visualization\n",
        "def inference(args):   \n",
        "    # get the RAFT model\n",
        "    model = RAFT(args)    \n",
        "    # load pretrained weights\n",
        "    pretrained_weights = torch.load(args.model)   \n",
        "    save = args.save\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        # parallel between available GPUs\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        # load the pretrained weights into model\n",
        "        model.load_state_dict(pretrained_weights)\n",
        "        model.to(device)\n",
        "       \n",
        "    # change model's mode to evaluation\n",
        "    model.eval() \n",
        "    video_stream()\n",
        "    label_html = ''\n",
        "    bbox = ''  \n",
        "    rows=args.patch[0]\n",
        "    cols=args.patch[1]\n",
        "    roi_original=args.roi\n",
        "    roi_n=new_roi(args.ratio, args.roi)\n",
        "    # capture the video and get the first frame\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    frame_1 = js_to_image(js_reply[\"img\"])\n",
        "    counter =1\n",
        "    #fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width=frame_1.shape[1]\n",
        "    frame_height=frame_1.shape[0]\n",
        "    dim = (int(frame_width*args.ratio),int(frame_height*args.ratio))\n",
        "    frame_1 = cv2.resize(frame_1, dim, interpolation = cv2.INTER_AREA)\n",
        "    #Patches borders on the original frames\n",
        "    global ox1\n",
        "    ox1=[]\n",
        "    global ox2\n",
        "    ox2=[]\n",
        "    global oy1\n",
        "    oy1=[]\n",
        "    global oy2\n",
        "    oy2=[] \n",
        "    ox1,oy1,ox2,oy2=patchBorder(rows, cols, args.roi[0],args.roi[1],args.roi[2],args.roi[3] )\n",
        "    #Cropped ROI\n",
        "    frame_1=frame_1[roi_n[1]:roi_n[3],roi_n[0]:roi_n[2]]\n",
        "    global x1\n",
        "    x1=[]\n",
        "    global x2\n",
        "    x2=[]\n",
        "    global y1\n",
        "    y1=[]\n",
        "    global y2\n",
        "    y2=[] \n",
        "    x1,y1,x2,y2=patchBorder(rows, cols, 0,0,int(frame_1.shape[1]), int(frame_1.shape[0]) )\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        begin=time.time()\n",
        "        js_reply = video_frame(label_html, bbox)\n",
        "        frame_1 = js_to_image(js_reply[\"img\"])\n",
        "        frame_1=frame_1[roi_n[1]:roi_n[3],roi_n[0]:roi_n[2]]\n",
        "        frame_1 = frame_preprocess(frame_1, device)\n",
        "       \n",
        "        while True:\n",
        "            finish=time.time()\n",
        "            if (finish-begin)>1.9:  \n",
        "                begin=finish\n",
        "                js_reply = video_frame(label_html, bbox)\n",
        "               \n",
        "                if not js_reply:\n",
        "                    break \n",
        "                frame_2 = js_to_image(js_reply[\"img\"])\n",
        "              \n",
        "                frame_2=frame_2[roi_n[1]:roi_n[3],roi_n[0]:roi_n[2]]              \n",
        "                frame_2 = frame_preprocess(frame_2, device)\n",
        "                flow_low, flow_up = model(frame_1, frame_2, iters=12, test_mode=True)\n",
        "                ret = vizualize_flow(frame_1, flow_up, save, counter)              \n",
        "                pushing_patches=ret\n",
        "                bbox_array = np.zeros([frame_height,frame_width,4], dtype=np.uint8)\n",
        "                for (x,y,xx,yy) in pushing_patches:\n",
        "                      bbox_array = cv2.rectangle(bbox_array,(x,y),(xx,yy),(255,0,0),2)\n",
        "                bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "                # convert overlay of bbox into bytes\n",
        "                bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "                # update bbox\n",
        "                bbox = bbox_bytes\n",
        "                ##########################################\n",
        "                frame_1 = frame_2 \n",
        "                counter += 1\n",
        "                           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TRSRSjAFmhs"
      },
      "source": [
        "# Start/Main\n",
        "Just you need to determine the ROI coordintes, and number of rows and columns patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHoRQGWUYSKc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "267fdf00-dce9-412b-ffec-a19378c655fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  /////////////////////////////////////////////////\n",
              "  ///////////////////User Inputs///////////////////\n",
              "    var w=960 // window width\n",
              "    var h=540 // window height\n",
              "  ////////////////////////////////////////////////\n",
              "    var w_css=w+'px'\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, w, h);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "     \n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = w_css;\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span></span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = '';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = w; //video.videoWidth;\n",
              "      captureCanvas.height = h; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "      \n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    } \n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Pre-trained RAFT model\n",
        "model_name=home_path+\"models/raft-sintel.pth\"\n",
        "ratio_v=1\n",
        "#Number of rows and columns patches\n",
        "#patch=[2,4]\n",
        "#ROI Coordinates\n",
        "#entrance_2\n",
        "#roi=[107,55,669,270]\n",
        "\n",
        "#150 \n",
        "#roi=[182,100,689,625]\n",
        "#270 and 280\n",
        "#roi=[187,165,695,535]\n",
        "#110\n",
        "#roi=[187,274,691,432]\n",
        "parser = ArgumentParser()\n",
        "parser.add_argument(\"--model\", help=\"restore checkpoint\",type=str, default=model_name)\n",
        " \n",
        "parser.add_argument(\"--save\", action=\"store_true\", help=\"save demo frames\", default=True)\n",
        "parser.add_argument(\"--small\", action=\"store_true\", help=\"use small model\", default=False)\n",
        "parser.add_argument(\"--ratio\", type=int, default=ratio_v)\n",
        "parser.add_argument('--roi','--list', nargs='+',type=int, help='<Required> Set flag',  default=roi)\n",
        "parser.add_argument('--patch','--list1', nargs='+',type=int, help='<Required> Set flag',  default=patch)\n",
        "parser.add_argument(\n",
        "        \"--mixed_precision\", action=\"store_true\", help=\"use mixed precision\"\n",
        "    )    \n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "start=time.time()\n",
        "inference(args)\n",
        "end=time.time()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}